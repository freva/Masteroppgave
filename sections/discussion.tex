%!TEX root = ../report.tex
\chapter{Discussion}
\label{cha:discussion}
In this chapter we evaluate our accomplishments, draw conclusions based on the conducted experiments and suggest possible future work. 

\section{Evaluation}
\label{sec:evaluation}
During the initial phase of this Master's Thesis work, four goals were formulated and described in Section~\ref{sec:project_goals}. In this section we evaluate to which degree each goal has been accomplished and discuss the overall quality of the developed systems. 

\subsection*{G1: Research Automatic Creation of Sentiment Lexica}
The field of automatic creation of sentiment lexica has been explored, in search of the most commonly used and best performing approaches. The research resulted in the identification of two main approaches: a graph approach and a \ac{pmi} approach. The information gathered and the knowledge gained in the field formed the basis for our work with our third goal (G3). 

\subsection*{G2: Research Lexicon Based Sentiment Analysis}
In addition to the research done to reach our first goal, we also studied the field of lexicon based \ac{sa}. Although the use of sentiment lexica is widespread within the field of \ac{sa}, we found that most \ac{sa} systems only use sentiment lexica as a feature among a series of features instead of basing the system around them which was what we wanted to explore. We were, however, able to identify interesting ideas and methods from the few lexicon based \ac{sa} systems we discovered. In addition to the more common uses of sentiment lexica within \ac{sa}, the ideas and methods from the lexicon based systems acted as a basis for the work on our fourth goal (G4).  

\subsection*{G3: Create a Twitter Specific Sentiment Lexicon}
By developing a sentiment lexica creator utilizing the core \ac{pmi} approach discovered in our research, a Twitter specific sentiment lexicon was successfully created. After two months of experimenting with a graph propagation approach without satisfactory results, we decided to try out the \ac{pmi} approach which instantly yielded better results, as described in Section~\ref{sec:transition_graph_pmi} and shown in Table~\ref{tab:graph_vs_pmi}. From that point on, the lexicon creator utilizing the \ac{pmi} approach became our main focus regarding the task of sentiment lexica creation. \\

The best performing sentiment lexicon we were able to create with our sentiment lexica creator, was a lexicon with 3\thinspace000 entries consisting of $n$-grams with $n\leq5$ selected based on their \ac{pmi} score and occurrence frequency. Compared to the previously created sentiment lexica, \textit{Sentiment140} and \textit{AFINN}, our lexicon seemingly outperforms the other two, as shown in Table~\ref{tab:lexicon_comparison}. However, as described in Section~\ref{sec:lexicon_comparison}, because our system is unable to utilize all features of the \textit{Sentiment140} lexicon no clear conclusion can be drawn based on the results. The performance of our lexicon against the \textit{AFINN} lexicon on the other hand is a more valid comparison. The fact that our lexicon outperforms the \textit{AFINN} lexicon, which is a commonly used manually annotated sentiment lexicon within \ac{sa}, shows that the overall quality of our lexicon is high.  

\subsection*{G4: Create a Lexicon Based Sentiment Analysis System}
A lexicon based sentiment analysis system/lexicon based classifier has been created. The classifier was developed in parallel with our lexicon creator, and is therefore specifically tailored to work well with our sentiment lexicon. Through the results of the performance tests described in Section~\ref{sec:system_performance}, we can see that the classifier utilizing our best performing sentiment lexicon almost keeps up with the two best performing comparison systems, the system by \citeauthor{FaretReitan} and the Initial Experiment, both utilizing \ac{svm} in the classification process. While achieving such good results, regarding F1-score, recall and precision our lexicon classifier completely outperforms the other systems in terms of run-time performance which along with the other performance measures has been a focus area throughout the development process. \\

When utilized as a feature in our Initial Experiment system we see a significant boost in performance. The classifier proves that its able to provide the \ac{svm} in our Initial Experiment system with additional and relevant information, further verifying its overall quality. As discovered in the ablation study described in Section~\ref{sec:ablation_study_lexicon_classifier}, the performance of the classifier is highly dependent on the quality of the lexicon it uses, and not so much on its different features. Good results for our classifier therefore also point to a high quality sentiment lexica.


\section{Conclusion}
\label{sec:conclusion}
Through the various experiments described in Section~\ref{sec:system_performance} we have discovered that the \ac{pmi} lexicon creation approach works well, but that the quality of the created lexicon is highly dependent on the quality of the large labeled tweet dataset. Acquiring a large labeled dataset of high quality, that is, a diverse dataset capturing as many aspects of the language as possible, is a difficult task. From our experiment of comparing different sized labeled datasets, described in Section~\ref{sec:labeled_dataset_size_comparison}, we see that once the dataset reaches a certain size, the performance of the resulting lexicon reaches its maximum only limited by the quality of the dataset. By using the \textit{AFINN} lexicon, the labeled dataset is limited to contain tweets with words and phrases found in the lexicon. Possible new and interesting lexicon words or phrases will only be found if they happen to also be part of a tweet containing enough words in \textit{AFINN}. \\

Another interesting result further supporting the \ac{pmi} lexicon creation approach and the quality of our lexicon and lexicon classifier, is the lexicon comparison results described in Section~\ref{sec:lexicon_comparison}. Our fully automatically created \ac{pmi} lexicon actually beats the manually annotated \textit{AFINN} lexicon on all datasets across all performance measures, which is both an important result and a noticeable feat. In addition to verifying the quality of our lexicon it also proves that creating sentiment lexica automatically is a highly viable sentiment lexicon creation method. \\

Throughout the development process of our lexicon creation system and our lexicon based classifier, as well as through the various experiments conducted in Chapter~\ref{cha:experiments}, the tight connection between the sentiment lexicon and the system utilizing the lexicon has become apparent. For a classifier to utilize a sentiment lexicon's full potential, the classifier must be specifically tailored to work with that specific lexicon. This is a consequence of the different sentiment lexica creation methods, where the creators of the different available lexica apply different features to their lexicon specifically meant to work well in another system or classifier. That is, there is no standardized format for automatically created sentiment lexica. \\

Based on our initial research within the field of automatic creation of sentiment lexica, we assumed that one of the feats of automatically created sentiment lexica would be that their size and word coverage would benefit their performance compared to manually annotated lexica. However, after our experiments of comparing lexica of various sizes, described in Section~\ref{sec:lexicon_size_comparison}, our assumption was not verified. Larger lexica did lead to a better coverage as shown in Table~\ref{tab:lexicon_coverage}, but their classification performance was not improved accordingly. Although our results point to that larger lexica with high coverage would not perform better than relatively small lexica with medium coverage, no definite conclusions can be drawn. The fact that larger lexica created with our lexicon creator did not lead to better performance might also be caused by the quality of the labeled dataset used, described earlier in this section. \\

Regarding the run-time performance of lexicon based \ac{sa} systems compared to more sophisticated \ac{sa} systems, our results clearly suggest that lexicon based systems are the most viable \ac{sa} systems to use in real-time classification applications. With our lexicon based classifier, we achieve a classification speed of 108\thinspace600 tweets per second, meaning that our system could have classified all of the 500 million tweets posted on Twitter each day in real-time 19 times over. With this result it would be possible to add more advanced features to our classifier, trading off run-time performance for better classification performance and still classify fast enough to be a real-time classifier.


\section{Future Work}
\label{sec:future_work}
Throughout the work on this Master's Thesis, a series of possible improvements for our developed systems along with other future work have been discovered. \\

\subsection{Multidimensional Lexicon}
The source of most of our misclassifications are the tweets with sentiment value of 0. In our current implementation, all these tweets are classified as neutral by default. We think it should be possible to extract several other values for each $n$-gram to the lexicon that can actually help classify a tweet as neutral instead of classifying all tweets with sentiment value of 0 as neutral. For example, each word could have a sentiment score and an objectivity score. The sentiment score would be calculated as it is today, while the objectivity score would be calculated also using the PMI approach, but on a dataset labeled as subjective/objective. 

\subsection{PMI approach}
When counting the positive and negative contexts for an $n$-gram in our \ac{pmi} approach to lexicon creation, negation is not handled. That is, if a tweet as a whole is labeled as positive, but there exists negated $n$-grams within the tweet, we still increment the positive context counter of the $n$-grams. However, we believe that the performance of the final lexicon might benefit from handling the negated $n$-grams differently in this counting process. This can be done by, for example, incrementing the opposite context counter or perhaps by incrementing the context counter by a smaller amount if the $n$-gram is negated. \\

\subsection{Graph Approach}
Because our main focus changed from the graph propagation approach to the \ac{pmi} approach for lexicon creation, some of the more advanced features or improvements of the \ac{pmi} lexicon were never incorporated into the graph propagation approach. It would therefore be interesting to see how and if \ac{pmi} $n$-grams, as opposed to the occurrence frequency $n$-grams we used, affect the overall performance of the graph propagation approach.\\

Another, perhaps more important improvement is how the similarity between the different context vectors is calculated. In our approach, the similarity measure cosine similarity is used. However, the similarity measure we believe would work best is the Pearson Correlation similarity measure, which is the fourth and final step in the COALS method where we only follow the first three. \\

\subsection{Combined Approach}
Where the \ac{pmi} lexicon creation approach is only concerned with finding the sentiment values of $n$-grams, the graph propagation approach is most concerned with the relationship between the different $n$-grams. The graph propagation approach namely strives to find similar $n$-grams. That is, $n$-grams used in similar contexts. Because one of the problems of the graph propagation approach is to find a good seed set with appropriate weights, it could be interesting to explore a lexicon creation system combining the \ac{pmi} approach and the graph propagation approach. The \ac{pmi}-approach could be used to identify a seed set with sentiment values, while the graph propagation approach could be used to find $n$-grams similar to the ones already present in the seed set. With more appropriate seed set values, the most similar $n$-grams found during graph propagation would hopefully be assigned more appropriate sentiment values themselves. \\ 

\subsection{Lexicon Extension}
Another interesting possible improvement is to extend the adjective and adverb addition. In addition to deriving the adverb and the missing forms of an adjective, one can explore the possibilities of adding the different forms of verbs. The verb \textit{love} for example, has the forms \textit{loves}, \textit{loved} and \textit{loving}, which could possibly be added to a lexicon if not already present. Exploring how to set the sentiment values of the missing verb forms would then also need to be done. One possible approach would be to set the sentiment value of a missing word to the initial word already found in the lexicon, but only add words if the initial word has an absolute sentiment value above a set threshold. \\

A different approach entirely could be to use a synonym dictionary. That way, synonyms of words already found in the lexicon could be added to expand and hopefully improve the overall coverage of the lexicon. The synonyms found could be given the same sentiment value as their related $n$-gram. This approach in combination with the aforementioned approach could be especially interesting.\\

\subsection{Lexicon Based Classifier}
Regarding our lexicon based classifier, there are three features in particular we believe would be interesting to explore further: capitalized words, elongated words and more special weights on non-letter characters. In our classifier all tweets are transformed into lower-case, disregarding all capitalized words. Exploring how one can utilize the use of capitalized words in a lexicon based classifier would therefore be interesting. \\

Although almost all elongated words are corrected, the fact that a word indeed was elongated is not utilized in the classification process. Similarly to the capitalized words, elongated words are also often used to boost sentiment value. A possible approach to utilizing the use of elongation could be to multiply the sentiment value of the corrected elongated word as a function of excess repeated characters in the word. \\

Almost all non-letter characters except "!" and "?" are removed in our system. Looking into such as the use of quotation and repeated use of the punctuation character "." and how they might affect the overall sentiment could also be particularly valuable. 

\glsresetall